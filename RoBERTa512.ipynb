{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa Model Implementation for Classification (using sliding window approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyman/Documents/Deep Learning/LAW-AI classification/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/spyman/Documents/Deep Learning/LAW-AI classification/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(texts, tokenizer, max_length=512, stride=256):\n",
    "    encodings = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for start in range(0, len(token_ids), stride):\n",
    "            end = min(start + max_length, len(token_ids))\n",
    "            input_ids = token_ids[start:end]\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            encodings.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    return encodings\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx // len(self.encodings[0])])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) * len(self.encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Input.csv') \n",
    "texts = df['Text'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, encoded_labels, test_size=0.1)\n",
    "\n",
    "train_encodings = sliding_window_tokenize(train_texts, tokenizer, max_length=512, stride=256)\n",
    "val_encodings = sliding_window_tokenize(val_texts, tokenizer, max_length=512, stride=256)\n",
    "\n",
    "train_dataset = SlidingWindowDataset(train_encodings, train_labels)\n",
    "val_dataset = SlidingWindowDataset(val_encodings, val_labels)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/342 [02:13<1:12:19, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3982, 'grad_norm': 2.9817054271698, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 20/342 [04:17<1:06:54, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3738, 'grad_norm': 3.302122116088867, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 30/342 [06:22<1:05:18, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3898, 'grad_norm': 3.591437816619873, 'learning_rate': 3e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 40/342 [08:33<1:05:40, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3964, 'grad_norm': 3.8775970935821533, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 50/342 [10:46<1:04:30, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3901, 'grad_norm': 1.9768247604370117, 'learning_rate': 5e-06, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 60/342 [12:53<1:00:44, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4101, 'grad_norm': 1.5315808057785034, 'learning_rate': 6e-06, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 70/342 [15:03<59:03, 13.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3934, 'grad_norm': 2.108271360397339, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 80/342 [17:12<56:26, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3863, 'grad_norm': 2.166874647140503, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 90/342 [19:20<53:23, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3957, 'grad_norm': 1.8043886423110962, 'learning_rate': 9e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 100/342 [21:29<52:12, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3949, 'grad_norm': 2.3684511184692383, 'learning_rate': 1e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 110/342 [23:36<48:41, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3849, 'grad_norm': 3.296285629272461, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 120/342 [41:08<17:50:55, 289.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3828, 'grad_norm': 2.4403011798858643, 'learning_rate': 1.2e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 130/342 [43:17<1:13:08, 20.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3931, 'grad_norm': 1.7376954555511475, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 140/342 [45:26<44:01, 13.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3809, 'grad_norm': 4.312652587890625, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 150/342 [47:34<41:10, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.403, 'grad_norm': 3.984259843826294, 'learning_rate': 1.5e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 160/342 [49:41<38:13, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3774, 'grad_norm': 2.781034469604492, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 170/342 [51:48<36:18, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3836, 'grad_norm': 2.667254686355591, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 180/342 [53:57<34:38, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.394, 'grad_norm': 3.5915427207946777, 'learning_rate': 1.8e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 190/342 [56:08<34:24, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3961, 'grad_norm': 4.310908317565918, 'learning_rate': 1.9e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 200/342 [58:19<29:42, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3843, 'grad_norm': 2.318768262863159, 'learning_rate': 2e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 210/342 [1:00:22<27:19, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3975, 'grad_norm': 2.069550037384033, 'learning_rate': 2.1e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 220/342 [1:02:23<24:00, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4081, 'grad_norm': 3.3788340091705322, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 230/342 [1:04:23<22:14, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3756, 'grad_norm': 2.529125213623047, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 240/342 [1:06:26<21:29, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3714, 'grad_norm': 2.5425479412078857, 'learning_rate': 2.4e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 250/342 [1:08:45<20:59, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3306, 'grad_norm': 4.75630521774292, 'learning_rate': 2.5e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 260/342 [1:10:48<17:04, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4111, 'grad_norm': 4.397022724151611, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 270/342 [1:13:11<18:27, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4042, 'grad_norm': 2.5951955318450928, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 280/342 [1:16:13<19:17, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3844, 'grad_norm': 4.611429214477539, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 290/342 [1:19:20<16:14, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3756, 'grad_norm': 2.026846170425415, 'learning_rate': 2.9e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 300/342 [1:22:15<11:06, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3904, 'grad_norm': 8.587235450744629, 'learning_rate': 3e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 310/342 [1:24:32<07:56, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3915, 'grad_norm': 4.809105396270752, 'learning_rate': 3.1e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 320/342 [1:27:32<04:57, 13.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.386, 'grad_norm': 2.7878432273864746, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 330/342 [1:29:34<02:26, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.393, 'grad_norm': 3.4179837703704834, 'learning_rate': 3.3e-05, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 340/342 [1:31:31<00:23, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3796, 'grad_norm': 2.6450376510620117, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [1:31:55<00:00, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5515.9664, 'train_samples_per_second': 0.496, 'train_steps_per_second': 0.062, 'train_loss': 1.388234747780694, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=342, training_loss=1.388234747780694, metrics={'train_runtime': 5515.9664, 'train_samples_per_second': 0.496, 'train_steps_per_second': 0.062, 'total_flos': 719884774342656.0, 'train_loss': 1.388234747780694, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Civil Case']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "model.save_pretrained('./fine-tuned-roberta512-sliding-window')\n",
    "tokenizer.save_pretrained('./fine-tuned-roberta512-sliding-window')\n",
    "\n",
    "def predict(texts):\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        encodings = sliding_window_tokenize([text], tokenizer, max_length=512, stride=256)\n",
    "        text_predictions = []\n",
    "        for enc in encodings:\n",
    "            inputs = {key: torch.tensor(val).unsqueeze(0) for key, val in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "            text_predictions.append(pred[0])\n",
    "        most_common_pred = Counter(text_predictions).most_common(1)[0][0]\n",
    "        predictions.append(most_common_pred)\n",
    "    return predictions\n",
    "\n",
    "example_texts = ['''A property owner files a lawsuit against their neighbor for encroaching on their land and demands that the boundary be restored to its rightful place.''']\n",
    "predictions = predict(example_texts)\n",
    "decoded_predictions = [label_encoder.inverse_transform([p])[0] for p in predictions]\n",
    "print(decoded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import torch\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./fine-tuned-roberta512-sliding-window')\n",
    "model = RobertaForSequenceClassification.from_pretrained('./fine-tuned-roberta512-sliding-window')\n",
    "label_encoder = joblib.load('label_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(texts, tokenizer, max_length=512, stride=256):\n",
    "    encodings = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for start in range(0, len(token_ids), stride):\n",
    "            end = min(start + max_length, len(token_ids))\n",
    "            input_ids = token_ids[start:end]\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            encodings.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Civil Case']\n"
     ]
    }
   ],
   "source": [
    "def predict(texts):\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        encodings = sliding_window_tokenize([text], tokenizer, max_length=512, stride=256)\n",
    "        text_predictions = []\n",
    "        for enc in encodings:\n",
    "            inputs = {key: torch.tensor(val).unsqueeze(0) for key, val in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "            text_predictions.append(pred[0])\n",
    "        most_common_pred = Counter(text_predictions).most_common(1)[0][0]\n",
    "        predictions.append(most_common_pred)\n",
    "    return predictions\n",
    "\n",
    "example_texts = ['''A property owner files a lawsuit against their neighbor for encroaching on their land and demands that the boundary be restored to its rightful place.''']\n",
    "predictions = predict(example_texts)\n",
    "decoded_predictions = [label_encoder.inverse_transform([p])[0] for p in predictions]\n",
    "print(decoded_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
